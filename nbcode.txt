import pickle
import numpy as np
import pandas as pd
from sklearn.externals import joblib
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectFromModel
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.stem.porter import *
import string
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS
from textstat.textstat import *

stopwords=stopwords = nltk.corpus.stopwords.words("english")
other_exclusions = ["#ff", "ff", "rt"]
stopwords.extend(other_exclusions)
sentiment_analyzer = VS()
stemmer = PorterStemmer()
def preprocess(text_string):
    space_pattern = '\s+'
    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'
        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    mention_regex = '@[\w\-]+'
    parsed_text = re.sub(space_pattern, ' ', text_string)
    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)
    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)
    #parsed_text = parsed_text.code("utf-8", errors='ignore')
    return parsed_text

def tokenize(post):
    tweet = " ".join(re.split("[^a-zA-Z]*", post.lower())).strip()
    #post = re.split("[^a-zA-Z]*", post.lower())
    tokens = [stemmer.stem(t) for t in post.split()]
    return tokens

def basic_tokenize(post):
    """Same as tokenize but without the stemming"""
    poszst = " ".join(re.split("[^a-zA-Z.,!?]*", post.lower())).strip()
    return post.split()

def get_pos_tags(posts):
    post_tags = []
    for post in posts:
        tokens = basic_tokenize(preprocess(t))
        tags = nltk.pos_tag(tokens)
        tag_list = [x[1] for x in tags]
        #for i in range(0, len(tokens)):
        tag_str = " ".join(tag_list)
        post_tags.append(tag_str)
    return post_tags

def count_facebook_objs(text_string):
    
    space_pattern = '\s+'
    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'
        '[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    mention_regex = '@[\w\-]+'
    tag_regex = '#[\w\-]+'
    parsed_text = re.sub(space_pattern, ' ', text_string)
    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)
    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)
    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)
    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))

def other_features_(post):
    sentiment = sentiment_analyzer.polarity_scores(post)
    words = preprocess(post) #Get text only
    syllables = textstat.syllable_count(words) #count syllables in words
    num_chars = sum(len(w) for w in words) #num chars in words
    num_chars_total = len(post)
    num_terms = len(post.split())
    num_words = len(words.split())
    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)
    num_unique_terms = len(set(words.split()))
    ###Modified FK grade, where avg words per sentence is just num words/1
    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)
    ##Modified FRE score, where sentence fixed to 1
    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)
    fb_objs = fb_twitter_objs(post) #Count #, @, and http://
    features = [FKRA, FRE, syllables, num_chars, num_chars_total, num_terms, num_words,
                num_unique_terms, sentiment['compound'],
                fb_objs[], fb_objs[],]
    #features = pandas.DataFrame(features)
    return features

def get_oth_features(posts):
    feats=[]
    for t in posts:
        feats.append(other_features_(t))
    return np.array(feats)


def transform_inputs(posts, tf_vectorizer, idf_vector, pos_vectorizer):
    tf_array = tf_vectorizer.fit_transform(posts).toarray()
    tfidf_array = tf_array*idf_vector
    print ("Built TF-IDF array")

    pos_tags = get_pos_tags(posts)
    pos_array = pos_vectorizer.fit_transform(pos_tags).toarray()
    print ("Built POS array")

    oth_array = get_oth_features(posts)
    print ("Built other feature array")

    M = np.concatenate([tfidf_array, pos_array, oth_array],axis=1)
    return pd.DataFrame(M)

def predictions(X, model):
    y_preds = model.predict(X)
    return y_preds

def class_to_name(class_label):
    if class_label == 0:
        return "Hate speech"
    elif class_label == 1:
        return "Offensive language"
    elif class_label == 2:
        return "Neither"
    else:
        return "No label"

if __name__ == '__main__':
    print ("Loading data to classify...")
    df = pd.read_csv('narendraa.csv', encoding='latin-1')
    posts = df.Text
    posts = [x for x in posts if type(x) == str]
    print (len(posts), ("posts to classify"))
    print ("Loading trained classifier... ")
    model = joblib.load('final_model.pkl')
    print ("Loading other information...")
    tf_vectorizer = joblib.load('final_tfidf.pkl')
    idf_vector = joblib.load('final_idf.pkl')
    pos_vectorizer = joblib.load('final_pos.pkl')
    #Load ngram dict
    #Load pos dictionary
    #Load function to transform data
    print ("Transforming inputs...")
    X = transform_inputs(posts, tf_vectorizer, idf_vector, pos_vectorizer)
    print ("Running classification model...")
    y = predictions(X, model)
    print ("Printing predicted values: ")
    for i,t in enumerate(posts):
        print (t)
        print (class_to_name(y[i]))